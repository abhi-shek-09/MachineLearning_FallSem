{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] \n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0\n",
    "y = y.reshape([150,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.bias1 = np.zeros((1, self.hidden_size))\n",
    "        self.bias2 = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            layer1 = X.dot(self.weights1) + self.bias1\n",
    "            activation1 = sigmoid(layer1)\n",
    "            layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "            activation2 = sigmoid(layer2)\n",
    "            \n",
    "            error = activation2 - y\n",
    "            d_weights2 = activation1.T.dot(error * sigmoid_derivative(layer2))\n",
    "            d_bias2 = np.sum(error * sigmoid_derivative(layer2), axis=0, keepdims=True)\n",
    "            error_hidden = error.dot(self.weights2.T) * sigmoid_derivative(layer1)\n",
    "            d_weights1 = X.T.dot(error_hidden)\n",
    "            d_bias1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "            \n",
    "            self.weights2 -= self.learning_rate * d_weights2\n",
    "            self.bias2 -= self.learning_rate * d_bias2\n",
    "            self.weights1 -= self.learning_rate * d_weights1\n",
    "            self.bias1 -= self.learning_rate * d_bias1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        layer1 = X.dot(self.weights1) + self.bias1\n",
    "        activation1 = sigmoid(layer1)\n",
    "        layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "        activation2 = sigmoid(layer2)\n",
    "        return (activation2 > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "mlp.fit(X, y)\n",
    "y_pred = mlp.predict(X)\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "Confusion Matrix:\n",
      "[[92  8]\n",
      " [ 0 50]]\n",
      "Precision: 0.86\n",
      "Sensitivity (Recall): 1.00\n",
      "F1 Score: 0.93\n",
      "ROC-AUC Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # Using petal length and petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0\n",
    "y = y.reshape([150, 1])\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.bias1 = np.zeros((1, self.hidden_size))\n",
    "        self.bias2 = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            layer1 = X.dot(self.weights1) + self.bias1\n",
    "            activation1 = sigmoid(layer1)\n",
    "            layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "            activation2 = sigmoid(layer2)\n",
    "            \n",
    "            error = activation2 - y\n",
    "            d_weights2 = activation1.T.dot(error * sigmoid_derivative(layer2))\n",
    "            d_bias2 = np.sum(error * sigmoid_derivative(layer2), axis=0, keepdims=True)\n",
    "            error_hidden = error.dot(self.weights2.T) * sigmoid_derivative(layer1)\n",
    "            d_weights1 = X.T.dot(error_hidden)\n",
    "            d_bias1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "            \n",
    "            self.weights2 -= self.learning_rate * d_weights2\n",
    "            self.bias2 -= self.learning_rate * d_bias2\n",
    "            self.weights1 -= self.learning_rate * d_weights1\n",
    "            self.bias1 -= self.learning_rate * d_bias1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        layer1 = X.dot(self.weights1) + self.bias1\n",
    "        activation1 = sigmoid(layer1)\n",
    "        layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "        activation2 = sigmoid(layer2)\n",
    "        return (activation2 > 0.5).astype(int)\n",
    "\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "mlp.fit(X, y)\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "sensitivity = recall_score(y, y_pred)\n",
    "f_score = f1_score(y, y_pred)\n",
    "roc_auc = roc_auc_score(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.2f}\")\n",
    "print(f\"F1 Score: {f_score:.2f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
