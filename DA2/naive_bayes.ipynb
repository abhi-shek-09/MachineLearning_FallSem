{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>Engine Type</th>\n",
       "      <th>Top Speed</th>\n",
       "      <th>Aerodynamics</th>\n",
       "      <th>Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Good</td>\n",
       "      <td>Red Bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Electric</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Mercedes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Red Bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Good</td>\n",
       "      <td>Red Bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Electric</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Mercedes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Color Engine Type Top Speed Aerodynamics      Team\n",
       "0   Red      Hybrid      Fast         Good  Red Bull\n",
       "1  Blue    Electric    Medium    Excellent  Mercedes\n",
       "2  Blue      Hybrid      Fast         Fair  Red Bull\n",
       "3   Red      Hybrid    Medium         Good  Red Bull\n",
       "4  Blue    Electric      Fast         Fair  Mercedes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('classification.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probs(df, target):\n",
    "    total = len(df)\n",
    "    class_counts = Counter(df[target])\n",
    "    class_probs = {i: ct / total for i, ct in class_counts.items()}\n",
    "    return class_counts, class_probs\n",
    "\n",
    "def feature_probs(df, feature, target):\n",
    "    feature_dict = {}\n",
    "    for class_ in df[target].unique():\n",
    "        mini_df = df[df[target] == class_]\n",
    "        feature_counts = Counter(mini_df[feature].astype(str))\n",
    "        tot_count = len(mini_df)\n",
    "        feature_dict[class_] = {f\"{val}\": count / tot_count for val, count in feature_counts.items()}\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "def calc_probs(instance, feat_probs, class_probs):\n",
    "    inst_probs = {}\n",
    "    for class_, class_prob in class_probs.items():\n",
    "        probs = class_prob\n",
    "        for i, feature_val in enumerate(instance):\n",
    "            if feature_val in feat_probs[i][class_]:\n",
    "                probs *= feat_probs[i][class_][feature_val]\n",
    "            else:\n",
    "                probs *= 0  \n",
    "        inst_probs[class_] = probs\n",
    "    return inst_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.columns[-1]\n",
    "class_counts, class_prob = class_probs(df, target)\n",
    "\n",
    "feature_probs_list = []\n",
    "for feature in df.columns:\n",
    "    if feature == target:\n",
    "        continue\n",
    "    feature_probs_list.append(feature_probs(df, feature, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Bull: 0.036000000000000004\n",
      "Mercedes: 0.0\n"
     ]
    }
   ],
   "source": [
    "prediction = \"Red,Electric,Fast,Good\"\n",
    "prediction = list(prediction.split(\",\"))\n",
    "val = calc_probs(prediction, feature_probs_list, class_prob)\n",
    "final_class = max(val, key=val.get)\n",
    "for i, j in val.items():\n",
    "    print(f\"{i}: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final prediction for the given input is: Red Bull\n"
     ]
    }
   ],
   "source": [
    "print(f\"The final prediction for the given input is: {final_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probabilities: {'Red Bull': 0.625, 'Mercedes': 0.375}\n",
      "Predictions for each row:\n",
      "        Team predicted_team\n",
      "0  Red Bull       Red Bull\n",
      "1  Mercedes       Mercedes\n",
      "2  Red Bull       Red Bull\n",
      "3  Red Bull       Red Bull\n",
      "4  Mercedes       Mercedes\n",
      "5  Mercedes       Mercedes\n",
      "6  Red Bull       Red Bull\n",
      "7  Red Bull       Red Bull\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall (Sensitivity): 1.00\n",
      "F1 Score: 1.00\n",
      "Specificity: 1.00\n",
      "ROC AUC Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Sample dataset as provided\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Blue', 'Red', 'Blue', 'Red', 'Blue', 'Red'],\n",
    "    'Engine Type': ['Hybrid', 'Electric', 'Hybrid', 'Hybrid', 'Electric', 'Hybrid', 'Electric', 'Hybrid'],\n",
    "    'Top Speed': ['Fast', 'Medium', 'Fast', 'Medium', 'Fast', 'Medium', 'Fast', 'Fast'],\n",
    "    'Aerodynamics': ['Good', 'Excellent', 'Fair', 'Good', 'Fair', 'Excellent', 'Good', 'Excellent'],\n",
    "    'Team': ['Red Bull', 'Mercedes', 'Red Bull', 'Red Bull', 'Mercedes', 'Mercedes', 'Red Bull', 'Red Bull']\n",
    "}\n",
    "df = pd.read_csv('classification.csv')\n",
    "\n",
    "def class_probs(df, target):\n",
    "    total = len(df)\n",
    "    class_counts = Counter(df[target])\n",
    "    class_probs = {i: ct / total for i, ct in class_counts.items()}\n",
    "    return class_counts, class_probs\n",
    "\n",
    "def feature_probs(df, feature, target):\n",
    "    feature_dict = {}\n",
    "    for class_ in df[target].unique():\n",
    "        mini_df = df[df[target] == class_]\n",
    "        feature_counts = Counter(mini_df[feature].astype(str))\n",
    "        tot_count = len(mini_df)\n",
    "        feature_dict[class_] = {f\"{val}\": count / tot_count for val, count in feature_counts.items()}\n",
    "    return feature_dict\n",
    "\n",
    "def calc_probs(instance, feat_probs, class_probs):\n",
    "    inst_probs = {}\n",
    "    for class_, class_prob in class_probs.items():\n",
    "        probs = class_prob\n",
    "        for i, feature_val in enumerate(instance):\n",
    "            if feature_val in feat_probs[i][class_]:\n",
    "                probs *= feat_probs[i][class_][feature_val]\n",
    "            else:\n",
    "                probs *= 0  \n",
    "        inst_probs[class_] = probs\n",
    "    return inst_probs\n",
    "\n",
    "target = df.columns[-1]\n",
    "class_counts, class_prob = class_probs(df, target)\n",
    "print(\"Class probabilities:\", class_prob)\n",
    "\n",
    "feature_probs_list = []\n",
    "for feature in df.columns[:-1]:  \n",
    "    feature_probs_list.append(feature_probs(df, feature, target))\n",
    "\n",
    "def predict(instance):\n",
    "    instance = list(instance)\n",
    "    val = calc_probs(instance, feature_probs_list, class_prob)\n",
    "    final_class = max(val, key=val.get)\n",
    "    return final_class\n",
    "\n",
    "df['predicted_team'] = df.apply(lambda row: predict(row[:-1]), axis=1)\n",
    "print(\"Predictions for each row:\\n\", df[['Team', 'predicted_team']])\n",
    "\n",
    "df['actual'] = df['Team'].apply(lambda x: 1 if x == 'Red Bull' else 0)\n",
    "df['predicted'] = df['predicted_team'].apply(lambda x: 1 if x == 'Red Bull' else 0)\n",
    "\n",
    "def evaluate_classification(df):\n",
    "    actual = df['actual']\n",
    "    predicted = df['predicted']\n",
    "\n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    precision = precision_score(actual, predicted)\n",
    "    recall = recall_score(actual, predicted)\n",
    "    f1 = f1_score(actual, predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix(actual, predicted).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    roc_auc = roc_auc_score(actual, predicted)\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "evaluate_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
