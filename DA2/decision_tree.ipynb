{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>Type</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Stolen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Imported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Color    Type    Origin Stolen\n",
       "0     Red  Sports  Domestic    Yes\n",
       "1     Red  Sports  Domestic     No\n",
       "2     Red  Sports  Domestic    Yes\n",
       "3  Yellow  Sports  Domestic     No\n",
       "4  Yellow  Sports  Imported    Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('classification.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_feature(df, feature, target):\n",
    "    values = df[feature].unique()\n",
    "    feature_count = {str(key): value for key, value in Counter(df[feature]).items()}\n",
    "    \n",
    "    count = {}\n",
    "    for val in values:\n",
    "        for tar in target:\n",
    "            count.setdefault(f\"{val},{tar}\", 0)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        count[str(row[feature]) + \",\" + str(row[df.columns[-1]])] += 1\n",
    "        \n",
    "    for ele in count:\n",
    "        count[ele] = (count[ele] / feature_count[ele.split(\",\")[0]])**2\n",
    "    final_dict = {}\n",
    "    for val in values:\n",
    "        total = 0\n",
    "        for ele in count:\n",
    "            if ele.split(\",\")[0] == val:\n",
    "                total += count[ele]\n",
    "        final_dict[val] = 1 - total\n",
    "    \n",
    "    for val in final_dict:\n",
    "        final_dict[val] = final_dict[val] * (feature_count[val]/len(df))\n",
    "    print(final_dict)\n",
    "    return sum(final_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Red': 0.24, 'Yellow': 0.24}\n",
      "0.48   1000000\n",
      "{'Sports': 0.26666666666666666, 'SUV': 0.15000000000000002}\n",
      "0.4166666666666667   0.48\n",
      "{'Domestic': 0.24, 'Imported': 0.24}\n",
      "0.48   0.4166666666666667\n",
      "Type\n"
     ]
    }
   ],
   "source": [
    "rootNode = \"\"\n",
    "min_val = 1000000\n",
    "target = list(df[df.columns[-1]].unique())\n",
    "for feature in df.columns[:-1]:\n",
    "    curr_val = gini_feature(df, feature, target)\n",
    "    print(curr_val , \" \" , min_val)\n",
    "    if curr_val < min_val:\n",
    "        min_val = curr_val\n",
    "        rootNode = feature\n",
    "\n",
    "print(rootNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected root feature: Aerodynamics\n",
      "  Color Engine Type Top Speed Aerodynamics      Team predicted_team\n",
      "0   Red      Hybrid      Fast         Good  Red Bull       Mercedes\n",
      "1  Blue    Electric    Medium    Excellent  Mercedes       Mercedes\n",
      "2  Blue      Hybrid      Fast         Fair  Red Bull       Mercedes\n",
      "3   Red      Hybrid    Medium         Good  Red Bull       Mercedes\n",
      "4  Blue    Electric      Fast         Fair  Mercedes       Mercedes\n",
      "5   Red      Hybrid    Medium    Excellent  Mercedes       Mercedes\n",
      "6  Blue    Electric      Fast         Good  Red Bull       Mercedes\n",
      "7   Red      Hybrid      Fast    Excellent  Red Bull       Mercedes\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.38\n",
      "Precision: 0.00\n",
      "Recall (Sensitivity): 0.00\n",
      "F1 Score: 0.00\n",
      "Specificity: 1.00\n",
      "ROC AUC Score: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91984\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('classification.csv')\n",
    "\n",
    "def gini_feature(df, feature, target_values):\n",
    "    values = df[feature].unique()\n",
    "    feature_count = {str(key): value for key, value in Counter(df[feature]).items()}\n",
    "    count = {}\n",
    "    for value in values:\n",
    "        for target_value in target_values:\n",
    "            count.setdefault(f\"{value} {target_value}\", 0)\n",
    "    for _, row in df.iterrows():\n",
    "        count[str(row[feature]) + \" \" + row[df.columns[-1]]] += 1\n",
    "    for ele in count:\n",
    "        count[ele] = (count[ele] / feature_count[ele.split()[0]])**2\n",
    "    \n",
    "    final_dict = {}\n",
    "    for feature_val in values:\n",
    "        total = 0\n",
    "        for ele in count:\n",
    "            if feature_val == ele.split(\" \")[0]:\n",
    "                total += count[ele]\n",
    "        final_dict[str(feature_val)] = 1 - total\n",
    "    \n",
    "    for val in final_dict:\n",
    "        final_dict[val] = final_dict[val] * (feature_count[val] / len(df))\n",
    "    return sum(final_dict.values())\n",
    "\n",
    "rootNode = \"\"\n",
    "min_val = float('inf')\n",
    "target_values = list(df[df.columns[-1]].unique())\n",
    "for feature in df.columns[:-1]:\n",
    "    curr_val = gini_feature(df, feature, target_values)\n",
    "    if curr_val < min_val:\n",
    "        rootNode = feature\n",
    "        min_val = curr_val\n",
    "print(\"Selected root feature:\", rootNode)\n",
    "\n",
    "df['predicted_team'] = df[rootNode].apply(lambda x: 'Red Bull' if x == 'Red' else 'Mercedes')\n",
    "df['actual'] = df['Team'].apply(lambda x: 1 if x == 'Red Bull' else 0)\n",
    "df['predicted'] = df['predicted_team'].apply(lambda x: 1 if x == 'Red Bull' else 0)\n",
    "\n",
    "def evaluate_classification(df):\n",
    "    actual = df['actual']\n",
    "    predicted = df['predicted']    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    # Precision\n",
    "    precision = precision_score(actual, predicted)\n",
    "    # Recall\n",
    "    recall = recall_score(actual, predicted)\n",
    "    f1 = f1_score(actual, predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix(actual, predicted).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    roc_auc = roc_auc_score(actual, predicted)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "evaluate_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
