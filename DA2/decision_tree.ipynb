{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>Engine Type</th>\n",
       "      <th>Top Speed</th>\n",
       "      <th>Aerodynamics</th>\n",
       "      <th>Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Good</td>\n",
       "      <td>Red Bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Electric</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Mercedes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Red Bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Good</td>\n",
       "      <td>Red Bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Electric</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Mercedes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Color Engine Type Top Speed Aerodynamics      Team\n",
       "0   Red      Hybrid      Fast         Good  Red Bull\n",
       "1  Blue    Electric    Medium    Excellent  Mercedes\n",
       "2  Blue      Hybrid      Fast         Fair  Red Bull\n",
       "3   Red      Hybrid    Medium         Good  Red Bull\n",
       "4  Blue    Electric      Fast         Fair  Mercedes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('classification.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_feature(df, feature, target_values):\n",
    "    values = df[feature].unique()\n",
    "    feature_count = {str(key): value for key, value in Counter(df[feature]).items()}\n",
    "    count = {}\n",
    "    for value in values:\n",
    "        for target_value in target_values:\n",
    "            count.setdefault(f\"{value} {target_value}\", 0)\n",
    "    for index, row in df.iterrows():\n",
    "        count[str(row[feature]) + \" \" + row[df.columns[-1]]] += 1\n",
    "    for ele in count:\n",
    "        count[ele] = (count[ele]/feature_count[ele.split()[0]])**2\n",
    "    \n",
    "    final_dict = {}\n",
    "    for feature_val in values:\n",
    "        total = 0\n",
    "        for ele in count:\n",
    "            if feature_val == ele.split(\" \")[0]:\n",
    "                total += count[ele]\n",
    "        final_dict[str(feature_val)] = 1 - total\n",
    "    \n",
    "    for val in final_dict:\n",
    "        final_dict[val] = final_dict[val] * (feature_count[val]/len(df))\n",
    "    return sum(final_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aerodynamics\n"
     ]
    }
   ],
   "source": [
    "rootNode = \"\"\n",
    "min_val = 1000000\n",
    "target_values = list(df[df.columns[-1]].unique())\n",
    "for feature in df.columns[:-1]:\n",
    "    curr_val = gini_feature(df, feature, target_values)\n",
    "    if curr_val < min_val:\n",
    "        rootNode = feature\n",
    "        min_val = curr_val\n",
    "print(rootNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected root feature: Aerodynamics\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.38\n",
      "Precision: 0.00\n",
      "Recall (Sensitivity): 0.00\n",
      "F1 Score: 0.00\n",
      "Specificity: 1.00\n",
      "ROC AUC Score: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91984\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('classification.csv')\n",
    "\n",
    "def gini_feature(df, feature, target_values):\n",
    "    values = df[feature].unique()\n",
    "    feature_count = {str(key): value for key, value in Counter(df[feature]).items()}\n",
    "    count = {}\n",
    "    for value in values:\n",
    "        for target_value in target_values:\n",
    "            count.setdefault(f\"{value} {target_value}\", 0)\n",
    "    for index, row in df.iterrows():\n",
    "        count[str(row[feature]) + \" \" + row[df.columns[-1]]] += 1\n",
    "    for ele in count:\n",
    "        count[ele] = (count[ele] / feature_count[ele.split()[0]])**2\n",
    "    \n",
    "    final_dict = {}\n",
    "    for feature_val in values:\n",
    "        total = 0\n",
    "        for ele in count:\n",
    "            if feature_val == ele.split(\" \")[0]:\n",
    "                total += count[ele]\n",
    "        final_dict[str(feature_val)] = 1 - total\n",
    "    \n",
    "    for val in final_dict:\n",
    "        final_dict[val] = final_dict[val] * (feature_count[val] / len(df))\n",
    "    return sum(final_dict.values())\n",
    "\n",
    "\n",
    "rootNode = \"\"\n",
    "min_val = float('inf')\n",
    "target_values = list(df[df.columns[-1]].unique())\n",
    "for feature in df.columns[:-1]:\n",
    "    curr_val = gini_feature(df, feature, target_values)\n",
    "    if curr_val < min_val:\n",
    "        rootNode = feature\n",
    "        min_val = curr_val\n",
    "print(\"Selected root feature:\", rootNode)\n",
    "\n",
    "df['predicted_team'] = df[rootNode].apply(lambda x: 'Red Bull' if x == 'Red' else 'Mercedes')\n",
    "\n",
    "df['actual'] = df['Team'].apply(lambda x: 1 if x == 'Red Bull' else 0)\n",
    "df['predicted'] = df['predicted_team'].apply(lambda x: 1 if x == 'Red Bull' else 0)\n",
    "\n",
    "def evaluate_classification(df):\n",
    "    actual = df['actual']\n",
    "    predicted = df['predicted']    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    # Precision\n",
    "    precision = precision_score(actual, predicted)\n",
    "    # Recall\n",
    "    recall = recall_score(actual, predicted)\n",
    "    f1 = f1_score(actual, predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix(actual, predicted).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    roc_auc = roc_auc_score(actual, predicted)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "evaluate_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
